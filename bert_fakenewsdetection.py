# -*- coding: utf-8 -*-
"""BERT_FakeNewsDetection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u0vAxXxmVkzGEWBZQjYnmJgXWfPUm5NT

# Fake News Detection

<!-- # EDA -->
<div id = "eda" style = "height: 50px;
  width: 800px;
  background-color: #813EEC;">
    <h1 style="padding: 10px;
              color:white;">
        <b>1.EDA</b>
    </h1>

</div>
"""

#importing Libraries
import numpy as np
import pandas as pd
from matplotlib.pylab import plt
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import confusion_matrix , classification_report
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from wordcloud import WordCloud
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import nltk

"""#        1.1 Reading Data
   
"""

df = pd.read_csv('/kaggle/input/fake-news-classification/WELFake_Dataset.csv')
df.head(120)

df.describe()

df.info()

y = df.label
print(f'Ratio of real and fake news:')
y.value_counts(normalize=True).rename({1: 'real', 0: 'fake'})

df.drop(["Unnamed: 0"], axis=1, inplace=True)

df.isnull().sum().plot(kind="barh")
plt.show()

"""# Observations:

         There are a total of 4 columns and 72134 rows in the data
         Label is the target variable
         Percentage of Real and fake News articles:
         real : 51%
         fake :49%
         Missing values are present in the dataset
   
"""

df.isnull().sum()

df = df.fillna('')

df.isnull().sum()

df.nunique()

df["title_text"] = df["title"] + df["text"]
df["body_len"] = df["title_text"].apply(lambda x: len(x) - x.count(" "))
df.head(50)

X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)

! pip install -U accelerate
! pip install -U transformers

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from transformers import BertTokenizer, BertTokenizerFast,BertForSequenceClassification, Trainer, TrainingArguments
from transformers import pipeline
import torch

# import BERT-base pretrained model
#bert = AutoModel.from_pretrained('bert-base-uncased')

# Load the BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

print(type(X_train.tolist()))
print(type(X_train.tolist()[0]))

a = X_train.tolist()
b = X_test.tolist()
train_encodings = tokenizer(a, truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(b, truncation=True, padding=True, max_length=512)

class FakeNewsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = FakeNewsDataset(train_encodings, y_train.tolist())
test_dataset = FakeNewsDataset(test_encodings, y_test.tolist())

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
# Move model to GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=2,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=lambda p: {"accuracy": accuracy_score(p.label_ids, p.predictions.argmax(-1))}
)

trainer.train()

# Save the model
model.save_pretrained('path_to_save_model')
tokenizer.save_pretrained('path_to_save_tokenizer')

# Load the model
model = BertForSequenceClassification.from_pretrained('path_to_save_model')
tokenizer = BertTokenizer.from_pretrained('path_to_save_tokenizer')

eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")

predictions = trainer.predict(test_dataset)
preds = predictions.predictions.argmax(-1)
print(classification_report(y_test, preds, target_names=['Fake', 'Real']))

plt.figure(figsize = (8,6))

sns.heatmap(confusion_matrix(y_test, preds.numpy()), annot=True,
            fmt='', cmap='Greens')

plt.xlabel('Predicted Labels')
plt.ylabel('Real Labels')

# Save the model
model.save_pretrained('path_to_save_model')
tokenizer.save_pretrained('path_to_save_tokenizer')

# Load the model
model = BertForSequenceClassification.from_pretrained('path_to_save_model')
tokenizer = BertTokenizer.from_pretrained('path_to_save_tokenizer')

